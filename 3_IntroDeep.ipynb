{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1053a6",
   "metadata": {},
   "source": [
    "# Motor Imagery EEG-BCIs - 0 to Deep Learning with BCI-IV 2a dataset\n",
    "\n",
    "@author: João Araújo ([LinkedIn](https://www.linkedin.com/in/joao-araujo-60470193/))\n",
    "\n",
    "## 3 - Intro to Deep Learning for BCIs: From Filterbank C(NN)SP to EEGNet\n",
    "Let's combine the previous conceptual ideas of what is going on on our EEG data to form an hypothesis that can guide our Machine Learning modelling strategy. When imagining different motor movements, a subject's EEG changes so that:\n",
    "\n",
    "- **Temporal Changes**: Specific rhythms of brain oscillations change (like ERS or ERD)\n",
    "- **Spatial Changes**: These changes happen differently across hemispheres or EEG channels.\n",
    "- **Classifier**: These changes might or might not have a linear relationship with MI class.\n",
    "\n",
    "For this Notebook, I also want to provide some evidence that we do not need so much pre-processing when applying these Deep Learning methods. So I encourage you to re-run the first Notebook after editing the last cell line:\n",
    "\n",
    "\n",
    "```\n",
    "# Get the event EEG data\n",
    "event_data = X_filtered[eeg_channels,triggers[i]:triggers[i] + mi_duration]\n",
    "```\n",
    "to:\n",
    "\n",
    "```\n",
    "# Get the event EEG data\n",
    "event_data = X[eeg_channels,triggers[i]:triggers[i] + mi_duration]\n",
    "```\n",
    "Once that is done and we have our new .mat, let's start with the imports and load functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9647ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import tensorflow as tf\n",
    "from keras.constraints import MaxNorm\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D,BatchNormalization, Dropout, Conv2D, Dense, Flatten, Input, Activation,AveragePooling2D, MaxPooling2D\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Load our data\n",
    "x = loadmat(\"BCI_IV_2a.mat\")\n",
    "y = np.squeeze(x['class_labels'])-1\n",
    "subj_labels = x['subj_labels']\n",
    "M_res = x['M_res']\n",
    "n_channels = np.size(M_res,axis = 2)\n",
    "del x\n",
    "\n",
    "# In case you have CUDA compatibility\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647e6ab",
   "metadata": {},
   "source": [
    "## Filterbank C(NN)SP\n",
    "We have previously covered CSP as a way to create spatial filters in a supervised manner. Filterbank CSP is an extension of the CSP methodology as it calculates CSP not for a single temporal filter, but for a filterbank (hence the name, I guess). This is a particularly useful method if we believe that different oscillation bands (alpha, beta, gamma, etc.) hold different spatial patterns between different MI classes. I have previously covered the reugular version of Filterbank CSP [here](https://github.com/joaoaraujo1/ML-BCI-dryEEG), so you can check it out for more details.\n",
    "\n",
    "Filterbank CSP however, suffers from a limitation: We need to define our bands of interest by hand, design the bandpass filters ourselves or choose an existing architecture. This means that we need to have previous knowledge about the frequency bands where the important information might be. That can obviously limit our model's accuracy if we are not right about which specific oscillatory bands can be useful or if we missed some bands. We can, however, work around this by implementing the same ideas of Filterbank CSP in a Convolutional Neural Network architecture, integrate it with a Logistic Regression classifier and train the whole thing in the same integrated optimization process. Here's how we do it using Tensorflow and Keras:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cedfd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FbC(NN)SP\n",
    "Filterbank CSP implemented as a CNN architecture\n",
    "Takes dictionary \"params\" as input:\n",
    "    params['t_filter_n']: number of temporal filters to calculate\n",
    "    params['t_filter_order']: order (kernel size-1) of the temporal filters\n",
    "    params['s_filter_n']: number of spatial filters to calculate\n",
    "    params['avg_samples']: number of timepoint samples to average oscillatory power\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def FBC_NN_SP(X: np.ndarray, params : dict, n_classes: int) -> keras.Model:\n",
    "    \n",
    "    ## Input and formatting ##\n",
    "    n_channels = X.shape[1]\n",
    "\n",
    "    input_shape = X.shape[1:]\n",
    "    \n",
    "    inputLayer = Input(input_shape)\n",
    "    \n",
    "    reshapeLayer = tf.expand_dims(inputLayer,axis=-1)  \n",
    "    \n",
    "    ## Temporal filtering ##\n",
    "    conv2dLayer_1 = Conv2D(filters = params['t_filter_n'], \n",
    "                            kernel_size= (1,params['t_filter_order']+1), \n",
    "                            activation='linear',\n",
    "                            use_bias = False,\n",
    "                            padding = 'valid')(reshapeLayer) \n",
    "    \n",
    "    ## Spatial filtering ##\n",
    "    depthconvLayer = DepthwiseConv2D(kernel_size= (n_channels,1), \n",
    "                                     activation='linear',\n",
    "                                     padding = 'valid',\n",
    "                                     depth_multiplier=params['s_filter_n'],\n",
    "                                     use_bias = False)(conv2dLayer_1) \n",
    "    \n",
    "    \n",
    "    ## Signal Power/Variance estimation ##\n",
    "    activationLayer_1 = Activation(activation = tf.math.square)(depthconvLayer)\n",
    "    poolLayer_1 = AveragePooling2D(pool_size=(1, conv2dLayer_1.shape[2]))(activationLayer_1) \n",
    "    \n",
    "    ## Logistic/Softmax Regression ##\n",
    "    flatFeatures = Flatten()(poolLayer_1)\n",
    "    if n_classes == 2:\n",
    "        outputLayer = Dense(units = 1,activation='sigmoid')(flatFeatures)\n",
    "    else:\n",
    "        outputLayer = Dense(units = n_classes,activation='softmax')(flatFeatures)        \n",
    "    \n",
    "    model = keras.Model(inputLayer,outputLayer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0260b87",
   "metadata": {},
   "source": [
    "Let's have an overview of each sequential operation on this model:\n",
    "\n",
    "0) **Input**: We input the model our `n_channels x n_timepoints matrix`.\n",
    "\n",
    "1) **Temporal Filtering**: Convolution across the temporal dimension of the signal. [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) is doing the work of a bandpass filter in a sense. We have to choose the number of filters and the order of the filter (but not the bandpass frequencies). Output shape: `t_filter_n x (n_timepoints - t_filter_order)`\n",
    "\n",
    "2) **Spatial Filtering**: Convolution across the EEG channel dimension. Mathematically this is similar to a CSP projection. We use [DepthwiseConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D) and not Conv2D, allowing each spatial filter to be tailored to its respective temporal filter. We can choose the number of spatial filters per temporal filter with `s_filter_n`. Output shape: `(t_filter_n * s_filter_n) x (n_timepoints - t_filter_order)`.\n",
    "\n",
    "3) **Signal Power/Variance estimation**: By using the square activation function followed by an average of the signal we are taking the power or basically the variance if the signal's mean = 0. These summary statistics of the temporally and spatially filtered signal will be used as the final features for our logistic classifier. Output shape: `(t_filter_n * s_filter_n) x 1`\n",
    "\n",
    "4) **Logistic/Softmax Regression**: We use [Dense](https://keras.io/api/layers/core_layers/dense/) to create a single fully connected layer to our logistic / softmax decision neuron(s).\n",
    "\n",
    "Let's print our model's structure, defining 4 temporal filters of order 4 with 2 spatial filters each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d655ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 22, 1000)]        0         \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda)  (None, 22, 1000, 1)      0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 22, 996, 4)        20        \n",
      "                                                                 \n",
      " depthwise_conv2d (Depthwise  (None, 1, 996, 8)        176       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1, 996, 8)         0         \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 1, 1, 8)          0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 232\n",
      "Trainable params: 232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Filterbank C(NN)SP parameters\n",
    "params = {\n",
    "    \"t_filter_n\": 4,\n",
    "    \"t_filter_order\":4,\n",
    "    \"s_filter_n\" : 2\n",
    "    }\n",
    "\n",
    "# Define the model\n",
    "fbcnnsp = FBC_NN_SP(X=M_res, params=params, n_classes=len(np.unique(y)))\n",
    "\n",
    "# Print model\n",
    "print(fbcnnsp.summary())                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee94e31",
   "metadata": {},
   "source": [
    "So here you go - an implementation of Filterbank CSP with a flavour of CNNs. While this is approach is interesting, it should be pointed out that:\n",
    "\n",
    "1) This model lacks a lot of ability to deal with nonlinear relationships between data and classifications. If we never used the *square* activation function, **our model could be summarized as a sequence of linear transformations**!\n",
    "\n",
    "2) When we average each spatiotemporal trial representation into a single summary statistic, we lose the ability to assess if earlier or later stages of the trial have different levels of importance - everything gets **dilluted into one single feature**.\n",
    "\n",
    "So let's go one step further and explore a famous CNN architecture for general EEG classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468cc59d",
   "metadata": {},
   "source": [
    "## EEGNet\n",
    "EEGNet ([paper](https://arxiv.org/abs/1611.08024)) takes the ideas of Filterbank CSP and adds another layer of complexity to it, check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EEGNet \n",
    "F1: Temporal filters\n",
    "D: number of spatial filters to learn per temporal filter F1\n",
    "F2: Compression (if F2 < D*F1) / Overcompletion (if F2 > D*F1) filters\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def EEGNet(X : np.ndarray ,F1 : int ,D : int ,F2 : int ,n_classes : int) -> keras.Model:\n",
    "    \n",
    "    ##### Block 1 #####\n",
    "    n_channels = X.shape[1]\n",
    "    \n",
    "    input_shape = X.shape[1:]\n",
    "    \n",
    "    inputLayer = Input(input_shape)\n",
    "    \n",
    "    reshapeLayer = tf.expand_dims(inputLayer,axis=-1)\n",
    "    \n",
    "    conv2dLayer_1 = Conv2D(filters = F1, \n",
    "                            kernel_size= (1,64), \n",
    "                            activation='linear',\n",
    "                            use_bias = False,\n",
    "                            padding = 'same')(reshapeLayer) \n",
    "    \n",
    "    normLayer_1 = BatchNormalization()(conv2dLayer_1)\n",
    "    \n",
    "    depthconvLayer = DepthwiseConv2D(kernel_size= (n_channels,1), \n",
    "                                     activation='linear',\n",
    "                                     padding = 'valid',\n",
    "                                     depth_multiplier= D,\n",
    "                                     use_bias = False,\n",
    "                                     kernel_constraint = MaxNorm(max_value = 1))(normLayer_1) \n",
    "    \n",
    "    normLayer_2 = BatchNormalization()(depthconvLayer)\n",
    "    \n",
    "    activationLayer_1 = Activation(activation = 'elu')(normLayer_2)\n",
    "    \n",
    "    poolLayer_1 = AveragePooling2D(pool_size=(1, 4))(activationLayer_1)\n",
    "    \n",
    "    dropLayer_1 = Dropout(rate = 0.25)(poolLayer_1) # 0.5 for within-subj\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Block 2 #####\n",
    "    \n",
    "    separableconvLayer = SeparableConv2D(filters=F2,\n",
    "                                         use_bias = 0,\n",
    "                                         kernel_size= (1,16), \n",
    "                                         activation='linear',\n",
    "                                         padding = 'same')(dropLayer_1)\n",
    "    \n",
    "    normLayer_3 = BatchNormalization()(separableconvLayer)\n",
    "    \n",
    "    activationLayer_2 = Activation(activation = 'elu')(normLayer_3)\n",
    "    \n",
    "    poolLayer_2 = AveragePooling2D(pool_size=(1, 8))(activationLayer_2)\n",
    "    \n",
    "    dropLayer_2 = Dropout(rate = 0.25)(poolLayer_2) # 0.5 for within-subj\n",
    "    \n",
    "        \n",
    "    ####\n",
    "    \n",
    "    flatFeatures = Flatten()(dropLayer_2)\n",
    "    if n_classes == 2:\n",
    "        outputLayer = Dense(units = 1,activation='sigmoid',kernel_constraint=MaxNorm(max_value=2))(flatFeatures)\n",
    "    else:\n",
    "        outputLayer = Dense(units = n_classes,activation='softmax',kernel_constraint=MaxNorm(max_value=2))(flatFeatures)        \n",
    "    \n",
    "    model = keras.Model(inputLayer,outputLayer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce8335d",
   "metadata": {},
   "source": [
    "While the first block is somewhat similar to Filterbank C(NN)SP, the main difference is the addition of a second block of operations that both filters each channel separately and mixes the result of these output channels. This is achieved through the [SeparableConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D) operation. The temporal filter order is fixed at 64 (half of the sampling frequency of the datasets the model was validated on), but we still need to choose the number of temporal filters (**F1**), spatial filters (**D**) and final mixed convolutions (**F2**). Other than that you can notice:\n",
    "\n",
    "1) Addition of 2 exponential linear activations to add non-linearity to the model;\n",
    "\n",
    "2) Usage of [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) as a regularization method (i.e. preventing overfitting);\n",
    "\n",
    "3) Sequential pooling of short sequences of the trial, allowing multiple features across different timepoints of each trial (versus single feature for Filterbank CSP)\n",
    "\n",
    "4) [Batch Normalization](https://keras.io/api/layers/normalization_layers/batch_normalization/) to help model convergence.\n",
    "\n",
    "Let's see how the transformations of a complete EEGNet with 8 temporal filters, 2 spatial filters per temporal filter looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80db347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 22, 1000)]        0         \n",
      "                                                                 \n",
      " tf.expand_dims_1 (TFOpLambd  (None, 22, 1000, 1)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 22, 1000, 8)       512       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 22, 1000, 8)      32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " depthwise_conv2d_1 (Depthwi  (None, 1, 1000, 16)      352       \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1, 1000, 16)      64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1, 1000, 16)       0         \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 1, 250, 16)       0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 250, 16)        0         \n",
      "                                                                 \n",
      " separable_conv2d (Separable  (None, 1, 250, 16)       512       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 1, 250, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1, 250, 16)        0         \n",
      "                                                                 \n",
      " average_pooling2d_2 (Averag  (None, 1, 31, 16)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 31, 16)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 496)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 1988      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,524\n",
      "Trainable params: 3,444\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "F1 = 8\n",
    "D = 2\n",
    "F2 = F1*D\n",
    "\n",
    "# Define model\n",
    "eegnet_82 = EEGNet(X=M_res ,F1 = F1 ,D = D ,F2 = F2 ,n_classes = 4) # EEGNet8,2\n",
    "\n",
    "# Check model summary\n",
    "print(eegnet_82.summary())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26213b40",
   "metadata": {},
   "source": [
    "### Model Validation 1 - Validation split of training set for early stoppage\n",
    "When training these architectures is good to put in place the best strategies to avoid overfitting and getting the best of our models. One of those strategies is splitting our training set so that part of it is used to validate each model in each fold on the fly and stop training before we overfit. In this implementation we can choose to split our validation set in `k` parts and we hold out a proportion `1/k` of the dataset to test it after every epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede55e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting training data into validation train/test splits\n",
    "## A proportion 1/k data is used for validation test\n",
    "def validationSplit(subj_labels,g_splits,i,k=3):\n",
    "    # Get our unique subjects\n",
    "    subject_splits = np.unique(subj_labels[g_splits[i][0]])\n",
    "    # Shuffle our unique subjects array\n",
    "    np.random.shuffle(subject_splits)\n",
    "    # Split them in k parts\n",
    "    splits = np.array_split(subject_splits,k)\n",
    "    # Use k-1 parts to train and 1 part to test\n",
    "    subj_test_val = splits[-1]\n",
    "    subj_train_val = np.concatenate(splits[:k-1])\n",
    "    # return train/test subject label arrays\n",
    "    return subj_train_val, subj_test_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295bd0b4",
   "metadata": {},
   "source": [
    "### Model Validation 2 - Group K-Fold for subjectwise classification\n",
    "If we want our predictions to generalize across subjects we should make sure that at every fold, our test data is gathered from a different subject. Like before, we use Leave-One-Participant-Out cross-validation. In Keras, we can get those cross-validation splits easily by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf50af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_kfold = GroupKFold(n_splits=9) # 9 participants total\n",
    "g_splits = list(group_kfold.split(M_res, y, subj_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e7fef",
   "metadata": {},
   "source": [
    "### Model Validation 3 - Boost your model validation speed with GPU (Optional)\n",
    "If you have access to a GPU that supports CUDA you can further optimize its memory management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae9633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory Management ##\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b90fc",
   "metadata": {},
   "source": [
    "## Model Comparisons\n",
    "We are now ready to test our two models and compare them. Will EEGNet beat the classic Filterbank CSP reinvented as a CNN? (This cell might take a while to run depending on your hardware availability, feel free to get a healthy snack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef188de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 11ms/step\n",
      "Subject 1 accuracy: 63.18%\n",
      "18/18 [==============================] - 0s 12ms/step\n",
      "Subject 2 accuracy: 50.27%\n",
      "18/18 [==============================] - 0s 11ms/step\n",
      "Subject 3 accuracy: 29.93%\n",
      "17/17 [==============================] - 0s 15ms/step\n",
      "Subject 4 accuracy: 70.17%\n",
      "17/17 [==============================] - 0s 15ms/step\n",
      "Subject 5 accuracy: 54.28%\n",
      "17/17 [==============================] - 0s 14ms/step\n",
      "Subject 6 accuracy: 49.72%\n",
      "16/16 [==============================] - 0s 9ms/step\n",
      "Subject 7 accuracy: 49.30%\n",
      "16/16 [==============================] - 0s 8ms/step\n",
      "Subject 8 accuracy: 53.88%\n",
      "14/14 [==============================] - 0s 15ms/step\n",
      "Subject 9 accuracy: 47.24%\n",
      "Model accuracy percent: 51.99% (var:1.10%)\n"
     ]
    }
   ],
   "source": [
    "model_type = 1 # 0 - FBC(NN)SP, 1- EEGNet\n",
    "\n",
    "## Useful variables to collect for AUC and accuracy\n",
    "total_acc = [] \n",
    "full_scores = []\n",
    "\n",
    "for i,(train_idc, test_idc) in enumerate(g_splits):\n",
    "    \n",
    "    # Get our test set and labels\n",
    "    X_test = M_res[test_idc,:,:]\n",
    "    y_test = y[test_idc] \n",
    "    \n",
    "    # Callback for early stopping\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                patience=10,\n",
    "                                                restore_best_weights = True)\n",
    "    \n",
    "    if model_type == 0:\n",
    "        model = FBC_NN_SP(X=M_res, params=params, n_classes=4) # FBC(NN)SP\n",
    "    elif model_type == 1:\n",
    "        model = EEGNet(X=M_res ,F1 = F1 ,D = D ,F2 = F2 ,n_classes = 4) # EEGNet8,2\n",
    "    \n",
    "    # Choose loss function to optimize\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Split training set to validation sets for early stoppage\n",
    "    subj_train_val, subj_test_val = validationSplit(subj_labels,g_splits,i,3)\n",
    "\n",
    "    X_train_val = M_res[ np.isin(subj_labels,subj_train_val),:,:]\n",
    "    y_train_val = y[np.isin(subj_labels,subj_train_val)]\n",
    "    X_test_val = M_res[ np.isin(subj_labels,subj_test_val),:,:]\n",
    "    y_test_val = y[np.isin(subj_labels,subj_test_val)]\n",
    "\n",
    "    # Fit\n",
    "    history = model.fit(x=X_train_val,\n",
    "                        y=y_train_val,\n",
    "                        epochs=100,\n",
    "                        batch_size = 32, \n",
    "                        shuffle = True,\n",
    "                        validation_data = [X_test_val,y_test_val],\n",
    "                        verbose = 0,\n",
    "                        callbacks = [callback])\n",
    "\n",
    "\n",
    "    # Classification as the highest proability prediction for test data\n",
    "    y_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_probs, axis = 1)\n",
    "\n",
    "    # Get accuract score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Subject %d accuracy: %.2f%%' %(i+1,accuracy*100))\n",
    "    i += 1\n",
    "\n",
    "    # Save each fold's accuracy\n",
    "    full_scores.append(accuracy)\n",
    "\n",
    "    ## Memory Management ##\n",
    "    del model, history\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "# Save mean accuracy\n",
    "total_acc.append([np.mean(full_scores),np.var(full_scores)])\n",
    "\n",
    "# Print\n",
    "print('Model accuracy percent: %.2f%% (var:%.2f%%)' %(total_acc[0][0]*100,total_acc[0][1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8fa83",
   "metadata": {},
   "source": [
    "And that is it. While your results may always differ a bit, we can always reach an accuracy level **better than that described on the EEGNet paper** with EEGNet combined with our method of validation and choice of loss function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016cc42",
   "metadata": {},
   "source": [
    "## Bonus: ShallowConvNet and DeepConvNet\n",
    "I have also implemented other architectures tested against EEGNet on the [paper](https://arxiv.org/abs/1611.08024) so you can play with and compare their performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6921e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ShallowConvNet\n",
    "For classification of oscillatory signals\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def ShallowConvNet(X : np.ndarray ,n_classes : int) -> keras.Model:\n",
    "    \n",
    "    n_channels = X.shape[1]\n",
    "\n",
    "    input_shape = X.shape[1:]\n",
    "    \n",
    "    inputLayer = Input(input_shape)\n",
    "    \n",
    "    reshapeLayer = tf.expand_dims(inputLayer,axis=-1)\n",
    "    \n",
    "    conv2dLayer_1 = Conv2D(filters = 40, \n",
    "                            kernel_size= (1,13), \n",
    "                            activation='linear',\n",
    "                            padding = 'same',\n",
    "                            kernel_constraint = MaxNorm(max_value = 2))(reshapeLayer) \n",
    "    \n",
    "    conv2dLayer_2 = Conv2D(filters = 40, \n",
    "                            kernel_size= (n_channels,1), \n",
    "                            activation='linear',\n",
    "                            padding = 'valid',\n",
    "                            use_bias = False,\n",
    "                            kernel_constraint = MaxNorm(max_value = 2))(conv2dLayer_1) \n",
    "    \n",
    "    normLayer = BatchNormalization(epsilon = 1e-5, \n",
    "                                   momentum = 0.1)(conv2dLayer_2)\n",
    "    \n",
    "    activationLayer_1 = Activation(activation = tf.math.square)(normLayer)\n",
    "    \n",
    "    poolLayer = AveragePooling2D(pool_size=(1, 35),strides=(1, 7))(activationLayer_1)\n",
    "    \n",
    "    activationLayer_2 = Activation(activation = tf.math.log)(poolLayer)\n",
    "    \n",
    "    flatFeatures = Flatten()(activationLayer_2)\n",
    "    \n",
    "    dropFeatures = Dropout(rate = 0.5)(flatFeatures)\n",
    "    \n",
    "    if n_classes == 2:\n",
    "        outputLayer = Dense(units = 1,activation='sigmoid',kernel_constraint=MaxNorm(max_value=2))(dropFeatures)\n",
    "    else:\n",
    "        outputLayer = Dense(units = n_classes,activation='softmax',kernel_constraint=MaxNorm(max_value=2))(dropFeatures)    \n",
    "    \n",
    "    model = keras.Model(inputLayer,outputLayer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "DeepConvNet\n",
    "For general purpose classification\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def DeepConvNet(X : np.ndarray ,n_classes : int) -> keras.Model:\n",
    "    \n",
    "    n_channels = X.shape[1]\n",
    "    \n",
    "    input_shape = X.shape[1:]\n",
    "\n",
    "    inputLayer = Input(input_shape)\n",
    "    \n",
    "    reshapeLayer = tf.expand_dims(inputLayer,axis=-1)\n",
    "    \n",
    "    conv2dLayer_1 = Conv2D(filters = 25, \n",
    "                            kernel_size= (1,5), \n",
    "                            activation='linear',\n",
    "                            padding = 'valid',\n",
    "                            kernel_constraint = MaxNorm(max_value = 2))(reshapeLayer) \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    conv2dLayer_2 = Conv2D(filters = 25, \n",
    "                            kernel_size= (n_channels,1), \n",
    "                            activation='linear',\n",
    "                            padding = 'valid',\n",
    "                            kernel_constraint = MaxNorm(max_value = 2))(conv2dLayer_1) \n",
    "    \n",
    "    normLayer_1 = BatchNormalization(epsilon = 1e-5, \n",
    "                                     momentum = 0.1)(conv2dLayer_2)\n",
    "    \n",
    "    activationLayer_1 = Activation(activation = 'elu')(normLayer_1)\n",
    "    \n",
    "    poolLayer_1 = MaxPooling2D(pool_size=(1, 2))(activationLayer_1)\n",
    "    \n",
    "    dropLayer_1 = Dropout(rate = 0.5)(poolLayer_1)\n",
    "    \n",
    "    #####\n",
    "    \n",
    "    conv2dLayer_3 = Conv2D(filters = 50, \n",
    "                            kernel_size= (1,5), \n",
    "                            activation='linear',\n",
    "                            padding = 'valid',\n",
    "                            kernel_constraint = MaxNorm(max_value = 2))(dropLayer_1) \n",
    "    \n",
    "    normLayer_2 = BatchNormalization(epsilon = 1e-5, \n",
    "                                     momentum = 0.1)(conv2dLayer_3)\n",
    "    \n",
    "    activationLayer_2 = Activation(activation = 'elu')(normLayer_2)\n",
    "    \n",
    "    poolLayer_2 = MaxPooling2D(pool_size=(1, 2))(activationLayer_2)\n",
    "    \n",
    "    dropLayer_2 = Dropout(rate = 0.5)(poolLayer_2)\n",
    "    \n",
    "    \n",
    "    #####\n",
    "    \n",
    "    \n",
    "    conv2dLayer_4 = Conv2D(filters = 100, \n",
    "                            kernel_size= (1,5), \n",
    "                            activation='linear',\n",
    "                            padding = 'valid',\n",
    "                            kernel_constraint = MaxNorm(max_value = 2))(dropLayer_2) \n",
    "    \n",
    "    normLayer_3 = BatchNormalization(epsilon = 1e-5, \n",
    "                                     momentum = 0.1)(conv2dLayer_4)\n",
    "    \n",
    "    activationLayer_3 = Activation(activation = 'elu')(normLayer_3)\n",
    "    \n",
    "    poolLayer_3 = MaxPooling2D(pool_size=(1, 2))(activationLayer_3)\n",
    "    \n",
    "    dropLayer_3= Dropout(rate = 0.5)(poolLayer_3)\n",
    "    \n",
    "    \n",
    "    ####\n",
    "    \n",
    "    \n",
    "    conv2dLayer_5 = Conv2D(filters = 200, \n",
    "                            kernel_size= (1,5), \n",
    "                            activation='linear',\n",
    "                            padding = 'valid',\n",
    "                            kernel_constraint = MaxNorm(max_value = 2))(dropLayer_3) \n",
    "    \n",
    "    normLayer_4 = BatchNormalization(epsilon = 1e-5, \n",
    "                                     momentum = 0.1)(conv2dLayer_5)\n",
    "    \n",
    "    activationLayer_4 = Activation(activation = 'elu')(normLayer_4)\n",
    "    \n",
    "    poolLayer_4 = MaxPooling2D(pool_size=(1, 2))(activationLayer_4)\n",
    "    \n",
    "    dropLayer_4 = Dropout(rate = 0.5)(poolLayer_4)\n",
    "    \n",
    "    \n",
    "    ####\n",
    "    \n",
    "    flatFeatures = Flatten()(dropLayer_4)\n",
    "    \n",
    "    if n_classes == 2:\n",
    "        outputLayer = Dense(units = 1,activation='sigmoid',kernel_constraint=MaxNorm(max_value=2))(flatFeatures)\n",
    "    else:\n",
    "        outputLayer = Dense(units = n_classes,activation='softmax',kernel_constraint=MaxNorm(max_value=2))(flatFeatures)\n",
    "\n",
    "    model = keras.Model(inputLayer,outputLayer)\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
